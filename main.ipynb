{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "cudnn.benchmark = True\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# System\n",
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import itertools\n",
    "# Custom\n",
    "import models\n",
    "import datasets\n",
    "from helpers import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-differential",
   "metadata": {},
   "source": [
    "# LCFCN Model\n",
    "\n",
    "Locates Objects with Point Supervision Training\n",
    "\n",
    "Based on https://github.com/ElementAI/LCFCN\n",
    "\n",
    "Experiment Settings below could eventually be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### EXPERIMENT SETTINGS ################################\n",
    "EXP_GROUPS = {}\n",
    "EXP_GROUPS['trancos'] = {\"dataset\": {'name': 'trancos', \n",
    "                                     'transform': 'rgb_normalize'},\n",
    "                         \"model\": {'name': 'lcfcn',\n",
    "                                   'base': \"fcn8_vgg16\"},\n",
    "                         \"batch_size\": [1, 5, 10],\n",
    "                         \"max_epoch\": [100],\n",
    "                         'dataset_size': [{'train': 'all', \n",
    "                                           'val': 'all'},],\n",
    "                         'optimizer': ['adam'],\n",
    "                         'lr':[1e-5]}\n",
    "\n",
    "EXP_GROUPS['trancos_debug'] = {\"dataset\": {'name': 'trancos', \n",
    "                                           'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'lcfcn',\n",
    "                                         'base': \"fcn8_vgg16\"},\n",
    "                               \"batch_size\": [1, 5, 10],\n",
    "                               \"max_epoch\": [5],\n",
    "                               'dataset_size': [{'train': 5, \n",
    "                                                 'val': 5},],\n",
    "                               'optimizer':['adam'],\n",
    "                               'lr':[1e-5]}\n",
    "\n",
    "EXP_GROUPS['denmark_debug'] = {\"dataset\": {'name': 'denmark', \n",
    "                                           'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'lcfcn',\n",
    "                                         'base': \"fcn8_vgg16\"},\n",
    "                               \"batch_size\": [1, 5, 10],\n",
    "                               \"max_epoch\": [5],\n",
    "                               'dataset_size': [{'train': 1, \n",
    "                                                 'val': 1},],\n",
    "                               'optimizer':['adam'],\n",
    "                               'lr':[1e-5]}\n",
    "\n",
    "EXP_GROUPS['trancos_debug_wtp'] = {\"dataset\": {'name': 'trancos', \n",
    "                                           'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'wtp',\n",
    "                                         'base': \"wtp_vgg16\"},\n",
    "                               \"batch_size\": [1, 20],\n",
    "                               \"max_epoch\": [5],\n",
    "                               'dataset_size': [{'train': 5, \n",
    "                                                 'val': 5},],\n",
    "                               'optimizer':['sdg'],\n",
    "                               'lr':[1e-5]}\n",
    "\n",
    "EXP_GROUPS = {k: utils.cartesian(v) for k, v in EXP_GROUPS.items()}\n",
    "\n",
    "exp_group_list = [\"denmark_debug\"]\n",
    "exp_list = []\n",
    "for exp_group_name in exp_group_list:\n",
    "    exp_list += EXP_GROUPS[exp_group_name]\n",
    "exp_dict = exp_list[0]\n",
    "    \n",
    "########################## FILE SYSTEM SETTINGS ###########################\n",
    "\n",
    "savedir_base = \"/home/jovyan/work/jannis/saves/LCFCN\"\n",
    "datadir = \"/home/jovyan/work/jannis/data/DENMARK\"\n",
    "\n",
    "############################### PRINTS ####################################\n",
    "\n",
    "pprint.pprint(exp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-going",
   "metadata": {},
   "source": [
    "## Saving Location\n",
    "\n",
    "Create new folder for the selected experiment and save the experiment dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = utils.hashDict(exp_dict) #generate ID by hashing experiment dict\n",
    "savedir = os.path.join(savedir_base, exp_id)\n",
    "\n",
    "# Backup and Overwrite previous experiment with same name\n",
    "utils.deleteExperiment(savedir, backup_flag = True)\n",
    "print(\"Cleared previous experiment...\")\n",
    "\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "utils.saveJSON(os.path.join(savedir, \"exp_dict.json\"), exp_dict)\n",
    "print(\"Experiment saved in %s\" % savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-helmet",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Introduce datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.getDataset(dataset_dict = exp_dict[\"dataset\"],\n",
    "                                 split = \"train\",\n",
    "                                 datadir = datadir,\n",
    "                                 exp_dict = exp_dict,\n",
    "                                 dataset_size = exp_dict['dataset_size'])\n",
    "val_set = datasets.getDataset(dataset_dict = exp_dict[\"dataset\"],\n",
    "                               split = \"val\",\n",
    "                               datadir = datadir,\n",
    "                               exp_dict = exp_dict,\n",
    "                               dataset_size = exp_dict['dataset_size'])\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set, replacement=True, num_samples=2*len(val_set))\n",
    "train_loader = DataLoader(train_set,\n",
    "                          sampler = train_sampler,\n",
    "                          batch_size = exp_dict[\"batch_size\"], \n",
    "                          drop_last = True, \n",
    "                          num_workers = 2)\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_set)\n",
    "val_loader = DataLoader(val_set,\n",
    "                        sampler = val_sampler,\n",
    "                        batch_size = 1,\n",
    "                        num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-generator",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Load Model and underlying base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.getModel(model_dict = exp_dict['model'],\n",
    "                         exp_dict = exp_dict,\n",
    "                         train_set = train_set).cuda()\n",
    "\n",
    "# model.opt = optimizers.get_optim(exp_dict['opt'], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-seven",
   "metadata": {},
   "source": [
    "## Experiment Run Management \n",
    "\n",
    "Resume experiment if a previous score_list exists or start a new one from epoch 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(savedir, \"model.pth\")\n",
    "score_list_path = os.path.join(savedir, \"score_list.pkl\")\n",
    "\n",
    "if os.path.exists(score_list_path): #resume\n",
    "    model.loadStateDict(utils.loadTorch(model_path))\n",
    "    score_list = utils.loadPKL(score_list_path)\n",
    "    s_epoch = score_list[-1]['epoch'] + 1\n",
    "    print(f\"Resuming previous experiment fom epoch {s_epoch}\")\n",
    "else: #restart\n",
    "    score_list = []\n",
    "    s_epoch = 0\n",
    "    print(f\"Beginning new experiment from epoch {s_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-poultry",
   "metadata": {},
   "source": [
    "## Main Epoch Loop\n",
    "\n",
    "Each epoch conists of training, validation, updating the statstics and saving the best as well as the most recent model and validation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(s_epoch, exp_dict['max_epoch']):\n",
    "    # Validate only at the start of each cycle\n",
    "    score_dict = {}\n",
    "    # Train the model\n",
    "    train_dict = model.trainOnLoader(model, train_loader)\n",
    "    print(\"Training done...\")\n",
    "    # Validate and Visualize the model\n",
    "    val_dict = model.valOnLoader(val_loader, savedir_images=os.path.join(savedir, \"images\"), n_images=3)\n",
    "    # model.visOnLoader(vis_loader, savedir=os.path.join(savedir, \"images\"))\n",
    "    print(\"Validation done..\")\n",
    "    \n",
    "    # Update score_dict and add to score_list\n",
    "    score_dict.update(val_dict)\n",
    "    score_dict.update(train_dict)\n",
    "    score_dict[\"epoch\"] = len(score_list)\n",
    "    score_list += [score_dict]\n",
    "\n",
    "    # Report score_list\n",
    "    score_df = pd.DataFrame(score_list)\n",
    "    print(\"\\n\", score_df.tail(), \"\\n\")\n",
    "    \n",
    "    # Save Model and score_list\n",
    "    utils.saveTorch(model_path, model.getStateDict())\n",
    "    utils.savePKL(score_list_path, score_list)\n",
    "    print(\"Checkpoint Saved: %s\" % savedir)\n",
    "\n",
    "    # Save best Checkpoint\n",
    "    if e == 0 or (score_dict.get(\"val_score\", 0) > score_df[\"val_score\"][:-1].fillna(0).max()):\n",
    "        utils.savePKL(os.path.join(savedir, \"score_list_best.pkl\"), score_list)\n",
    "        utils.saveTorch(os.path.join(savedir, \"model_best.pth\"), model.getStateDict())\n",
    "        print(\"Saved Best: %s\" % savedir)\n",
    "    print(f\"Epoch {e+1} of {exp_dict['max_epoch'] - s_epoch} completed.\")\n",
    "\n",
    "print(f\"Experiment completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
