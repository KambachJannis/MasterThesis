{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322d4e2c-4ff5-4ec2-84b6-9b37aedcdc8d",
   "metadata": {},
   "source": [
    "# OBJECTNESS EVALUATION\n",
    "\n",
    "The purpose of this Notebook is to evaluate the performance of different generic object proposal approaches on high-resultion satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073b12c7-c2b0-4a85-9ab5-8827e66b74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from helpers import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af140d-0fc7-403d-b7bf-5b04b78e2750",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ba53d2-0b4d-454a-83f5-877230351606",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/jovyan/work/DENMARK/512x512\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98059296-696d-49f4-bfeb-5a01c5296b2e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96db815-55a5-4adf-9d2b-df175ab37fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join(DATA_PATH, 'image_sets_trees', 'eval_shapes.txt')\n",
    "images = [name.replace(\"\\n\",\"\") for name in io.readText(images_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67f0912-c750-448c-9117-6b53d3159531",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.getDataset(name = \"denmark_shapes\", \n",
    "                              path = DATA_PATH,\n",
    "                              images = images,\n",
    "                              object_type = 'trees_eval',\n",
    "                              n_classes = 2,\n",
    "                              transform = None)\n",
    "\n",
    "sampler = torch.utils.data.RandomSampler(dataset)\n",
    "loader = DataLoader(dataset, sampler = sampler, batch_size = 1, drop_last = True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c9a3b-3ad1-42cf-a1b9-29bdde2beaa2",
   "metadata": {},
   "source": [
    "## Eval COB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54f730e-38e6-4e66-bf01-e9201adc1ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.cobnet import COBNet\n",
    "from helpers.cob.dataset import COBtransform\n",
    "\n",
    "means = [0.492, 0.475, 0.430]\n",
    "stds = [0.176, 0.173, 0.176]\n",
    "transform = COBtransform(means, stds, 512)\n",
    "\n",
    "cob_model = COBNet()\n",
    "cob_model.load_state_dict(torch.load(\"/home/jovyan/work/runs/X_COBNET/cp_or.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "624537a5-be91-4019-a949-82049ca37257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b701f2708e90467a823b25ad4a7b8fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COB Scores: \n",
      " -BCELoss: 0.46418856233358385\n",
      " -mIoU: 0.08872219261236919\n"
     ]
    }
   ],
   "source": [
    "cob_loss_running = 0\n",
    "cob_mIoU_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch[\"images\"].numpy()\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    # transforms\n",
    "    image_cob = transform(images = image)[0]\n",
    "    image_cob = np.stack((image_cob[:,:,0], image_cob[:,:,1], image_cob[:,:,2]), axis = 0)\n",
    "    cob_tensor = torch.tensor(image_cob[np.newaxis, ...]).float()\n",
    "    \n",
    "    # model run\n",
    "    cob_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_cob = cob_model(cob_tensor)\n",
    "    \n",
    "    cob_preds = out_cob['y_fine'].sigmoid().squeeze()\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(cob_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - cob_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        cob_loss_running += loss1.item()\n",
    "    else:\n",
    "        cob_loss_running += loss0.item()\n",
    "    \n",
    "    # eval with mIoU\n",
    "    shapes_mask = shapes == 1\n",
    "    cob_preds_np = torch.round(cob_preds).numpy()\n",
    "    cob_mask1 = cob_preds_np == 1\n",
    "    cob_mask0 = cob_preds_np == 0\n",
    "    intersection1 = np.logical_and(cob_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(cob_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(cob_mask1, shapes_mask).sum()\n",
    "        cob_mIoU_running += intersection1 / union\n",
    "    else:\n",
    "        union = np.logical_or(cob_mask0, shapes_mask).sum()\n",
    "        cob_mIoU_running += intersection0 / union\n",
    "        \n",
    "\n",
    "cob_loss = cob_loss_running / len(loader)\n",
    "cob_mIoU = cob_mIoU_running / len(loader)\n",
    "print(f'COB Scores: \\n -BCELoss: {cob_loss}\\n -mIoU: {cob_mIoU}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a3656-e17f-4738-b4c1-3851cd22ec7f",
   "metadata": {},
   "source": [
    "## Eval WTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a8d8fa-bd3f-4d2e-96b6-ff66787032f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness', nargout=0)\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness/pff_segment', nargout=0)\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness/MEX', nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bffb8052-a75f-47ce-b3a3-7ffa48876a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e0ef89fafb473cbaee69a19616f332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WTP Scores: \n",
      " -BCELoss: 0.5484897948801517\n",
      " -mIoU: 0.09084602931774151\n"
     ]
    }
   ],
   "source": [
    "wtp_loss_running = 0\n",
    "wtp_mIoU_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch['meta']['path'][0]\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    # matlab function run\n",
    "    wtp_preds = np.asarray(eng.getHeatMap(image, 100))\n",
    "    wtp_preds = torch.from_numpy(wtp_preds.astype(np.float32))\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(wtp_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - wtp_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        wtp_loss_running += loss1.item()\n",
    "    else:\n",
    "        wtp_loss_running += loss0.item()\n",
    "        \n",
    "    # eval with mIoU\n",
    "    shapes_mask = shapes == 1\n",
    "    wtp_preds_np = torch.round(wtp_preds).numpy()\n",
    "    wtp_mask1 = wtp_preds_np == 1\n",
    "    wtp_mask0 = wtp_preds_np == 0\n",
    "    intersection1 = np.logical_and(wtp_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(wtp_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(wtp_mask1, shapes_mask).sum()\n",
    "        wtp_mIoU_running += intersection1 / union\n",
    "    else:\n",
    "        union = np.logical_or(wtp_mask0, shapes_mask).sum()\n",
    "        wtp_mIoU_running += intersection0 / union\n",
    "        \n",
    "    \n",
    "wtp_loss = wtp_loss_running / len(loader)\n",
    "wtp_mIoU = wtp_mIoU_running / len(loader)\n",
    "print(f'WTP Scores: \\n -BCELoss: {wtp_loss}\\n -mIoU: {wtp_mIoU}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49fd9ba2-7010-4b5e-bee3-4a6961e8f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4651b-c409-4c67-b131-621a68e4d7c3",
   "metadata": {},
   "source": [
    "## Eval CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5273a-ac17-4f4d-a75d-70037b888f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load helper shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4017b64-9339-48a7-a471-c16f56d6125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_loss_running = 0\n",
    "cam_mIoU_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch['images'].numpy()\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    # run estimation method\n",
    "    cam_preds = ??? # needs to return [height x width] matrix in 0 - 1 range (sigmoid) as tensor\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(cam_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - cam_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        cam_loss_running += loss1.item()\n",
    "    else:\n",
    "        cam_loss_running += loss0.item()\n",
    "        \n",
    "    # eval with mIoU\n",
    "    shapes_mask = shapes == 1\n",
    "    cam_preds_np = torch.round(cam_preds).numpy()\n",
    "    cam_mask1 = cam_preds_np == 1\n",
    "    cam_mask0 = cam_preds_np == 0\n",
    "    intersection1 = np.logical_and(cam_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(cam_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(cam_mask1, shapes_mask).sum()\n",
    "        cam_mIoU_running += intersection1 / union\n",
    "    else:\n",
    "        union = np.logical_or(cam_mask0, shapes_mask).sum()\n",
    "        cam_mIoU_running += intersection0 / union\n",
    "        \n",
    "    \n",
    "cam_loss = cam_loss_running / len(loader)\n",
    "cam_mIoU = cam_mIoU_running / len(loader)\n",
    "print(f'WTP Scores: \\n -BCELoss: {cam_loss}\\n -mIoU: {cam_mIoU}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351253c-c1c5-4d30-acd7-aba37cae5080",
   "metadata": {},
   "source": [
    "# EXPERIMENTATION ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a05ba-ef55-441a-bb24-b8493c642267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(loader)\n",
    "batch = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e2f9c-ffdf-4653-a9c8-cb8e0847eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch[\"images\"].numpy()\n",
    "shapes = batch[\"shapes\"].squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6878783-0be6-495d-bbae-ad14dc74712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((3, 250, 250), requires_grad=True)\n",
    "target = torch.empty((3, 250, 250)).random_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4650d27-b4a1-4d95-ada6-148eecf1b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval with BCELoss\n",
    "criterion = torch.nn.BCELoss()\n",
    "loss = criterion(input.sigmoid(), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08169fea-b4a3-4286-ae30-9010ba9b5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = criterion(input.sigmoid()[0], target[0])\n",
    "loss2 = criterion(input.sigmoid()[1], target[1])\n",
    "loss3 = criterion(input.sigmoid()[2], target[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfb8c7-f384-4313-af9c-0ab6b1035943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "img = Image.new('L', (256, 256), 0)\n",
    "inb1 = np.array(img)\n",
    "inb2 = torch.from_numpy(inb1).long()\n",
    "inb3 = inb2.unsqueeze(0) # adds 1 dimension in front [256 x 256] -> [1 x 256 x 256]\n",
    "shape = inb3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cc82f-fdbe-4b3b-94bf-d546da63fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92268b1-6359-4c99-8373-34cb430ce6df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
