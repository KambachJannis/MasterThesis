{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322d4e2c-4ff5-4ec2-84b6-9b37aedcdc8d",
   "metadata": {},
   "source": [
    "# OBJECTNESS EVALUATION\n",
    "\n",
    "The purpose of this Notebook is to evaluate the performance of different generic object proposal approaches on high-resultion satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073b12c7-c2b0-4a85-9ab5-8827e66b74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.base.cobnet import COBNet\n",
    "from helpers.cob.dataset import COBtransform\n",
    "\n",
    "import datasets\n",
    "import helpers.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af140d-0fc7-403d-b7bf-5b04b78e2750",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ba53d2-0b4d-454a-83f5-877230351606",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/jovyan/work/DENMARK/256x256\"\n",
    "SAVE_PATH = \"/home/jovyan/work/runs/OBJECTNESS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98059296-696d-49f4-bfeb-5a01c5296b2e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96db815-55a5-4adf-9d2b-df175ab37fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join(DATA_PATH, 'image_sets', 'shapes.txt')\n",
    "images = [name.replace(\"\\n\",\"\") for name in utils.readText(images_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b67f0912-c750-448c-9117-6b53d3159531",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.getDataset(name = \"denmark_shapes\", \n",
    "                              path = DATA_PATH,\n",
    "                              images = images,\n",
    "                              n_classes = 2,\n",
    "                              transform = None)\n",
    "\n",
    "sampler = torch.utils.data.RandomSampler(dataset)\n",
    "loader = DataLoader(dataset, sampler = sampler, batch_size = 1, drop_last = True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c9a3b-3ad1-42cf-a1b9-29bdde2beaa2",
   "metadata": {},
   "source": [
    "Prepare COBNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d54f730e-38e6-4e66-bf01-e9201adc1ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = [0.492, 0.475, 0.430]\n",
    "stds = [0.176, 0.173, 0.176]\n",
    "transform = COBtransform(means, stds, 256)\n",
    "\n",
    "cob_model = COBNet()\n",
    "cob_model.load_state_dict(torch.load(\"/home/jovyan/work/runs/COBNET/cp_or.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d3a05ba-ef55-441a-bb24-b8493c642267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(loader)\n",
    "batch = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "295e2f9c-ffdf-4653-a9c8-cb8e0847eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch[\"images\"].numpy()\n",
    "shapes = batch[\"shapes\"].squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624537a5-be91-4019-a949-82049ca37257",
   "metadata": {},
   "outputs": [],
   "source": [
    "cob_loss_running = 0\n",
    "cob_mIoU_running = 0\n",
    "\n",
    "for batch in loader:\n",
    "    image = batch[\"images\"].numpy()\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    ##### WTP ######\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    ##### COB ######\n",
    "    image_cob = transform(images = image)[0]\n",
    "    image_cob = np.stack((image_cob[:,:,0], image_cob[:,:,1], image_cob[:,:,2]), axis = 0)\n",
    "    cob_tensor = torch.tensor(image_cob[np.newaxis, ...]).float()\n",
    "    \n",
    "    cob_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_cob = cob_model(cob_tensor)\n",
    "    \n",
    "    cob_preds = out_cob['y_fine'].sigmoid().squeeze()\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss = criterion(cob_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    cob_running += loss.item()\n",
    "    \n",
    "    # eval with mIoU\n",
    "    cob_mask = torch.round(cob_heatmap).numpy() == 1\n",
    "    shapes_mask = shapes == 1\n",
    "    union = np.logical_or(cob_mask, shapes_mask).sum()\n",
    "    intersection = np.logical_and(cob_mask, shapes_mask).sum()\n",
    "    cob_miou_running += intersection / union\n",
    "    \n",
    "    ##### CAM #####\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "\n",
    "cob_loss = cob_loss_running / len(loader)\n",
    "cob_mIoU = cob_mIoU_running / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351253c-c1c5-4d30-acd7-aba37cae5080",
   "metadata": {},
   "source": [
    "# EXPERIMENTATION ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c20844-fffe-4ba6-a924-9e058f93001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.492, 0.475, 0.430]\n",
    "std = [0.176, 0.173, 0.176]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), #convert image to Tensor\n",
    "    transforms.Normalize(mean = mean_new, std = std_new) #normalize Image Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ce5bc9-ea8a-495a-be00-2b838bf81247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy way to determine means and stds for entire dataset - get batch with all images\n",
    "loader = DataLoader(dataset, sampler = sampler, batch_size = len(dataset), drop_last = True, num_workers = 1)\n",
    "dataiter = iter(loader)\n",
    "batch = dataiter.next()\n",
    "print(np.mean(batch['images'].numpy(), axis = (0, 2, 3)), \"\\n\", np.std(batch['images'].numpy(), axis = (0, 2, 3)))\n",
    "# sanity check without axis\n",
    "print(np.mean(batch['images'].numpy()), \"\\n\", np.std(batch['images'].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f571e-d771-4b53-867f-a4b2f3bdd00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
