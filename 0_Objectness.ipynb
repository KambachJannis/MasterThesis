{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322d4e2c-4ff5-4ec2-84b6-9b37aedcdc8d",
   "metadata": {},
   "source": [
    "# OBJECTNESS EVALUATION\n",
    "\n",
    "The purpose of this Notebook is to evaluate the performance of different generic object proposal approaches on high-resultion satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b12c7-c2b0-4a85-9ab5-8827e66b74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from helpers import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af140d-0fc7-403d-b7bf-5b04b78e2750",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba53d2-0b4d-454a-83f5-877230351606",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/jovyan/work/mydata/DENMARK/512x512\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98059296-696d-49f4-bfeb-5a01c5296b2e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96db815-55a5-4adf-9d2b-df175ab37fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join(DATA_PATH, 'image_sets_trees', 'eval_shapes.txt')\n",
    "#images_path = os.path.join(DATA_PATH, 'image_sets_buildings', 'eval_shapes.txt')\n",
    "images = [name.replace(\"\\n\",\"\") for name in io.readText(images_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f0912-c750-448c-9117-6b53d3159531",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.getDataset(name = \"denmark_shapes\", \n",
    "                              path = DATA_PATH,\n",
    "                              images = images,\n",
    "                              object_type = 'trees_eval', #trees_eval or buildings\n",
    "                              n_classes = 2,\n",
    "                              transform = None)\n",
    "\n",
    "sampler = torch.utils.data.RandomSampler(dataset)\n",
    "loader = DataLoader(dataset, sampler = sampler, batch_size = 1, drop_last = True, num_workers = 1)\n",
    "\n",
    "print(f\"Loaded eval set of {len(loader)} images...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c9a3b-3ad1-42cf-a1b9-29bdde2beaa2",
   "metadata": {},
   "source": [
    "## Eval COB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f730e-38e6-4e66-bf01-e9201adc1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cobnet import COBNet\n",
    "from helpers.cob.dataset import COBtransform\n",
    "\n",
    "means = [0.492, 0.475, 0.430]\n",
    "stds = [0.176, 0.173, 0.176]\n",
    "transform = COBtransform(means, stds, 512)\n",
    "\n",
    "cob_model = COBNet()\n",
    "cob_model.load_state_dict(torch.load(\"/home/jovyan/work/runs/X_COBNET/cp_or.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624537a5-be91-4019-a949-82049ca37257",
   "metadata": {},
   "outputs": [],
   "source": [
    "cob_loss_running = 0\n",
    "cob_mIoU_running = 0\n",
    "cob_dice_running = 0\n",
    "cob_pAcc_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch[\"images\"].numpy()\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    # transforms\n",
    "    image_cob = transform(images = image)[0]\n",
    "    image_cob = np.stack((image_cob[:,:,0], image_cob[:,:,1], image_cob[:,:,2]), axis = 0)\n",
    "    cob_tensor = torch.tensor(image_cob[np.newaxis, ...]).float()\n",
    "    \n",
    "    # model run\n",
    "    cob_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_cob = cob_model(cob_tensor)\n",
    "    \n",
    "    cob_preds = out_cob['y_fine'].sigmoid().squeeze()\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(cob_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - cob_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        cob_loss_running += loss1.item()\n",
    "    else:\n",
    "        cob_loss_running += loss0.item()\n",
    "    \n",
    "    # eval with mIoU\n",
    "    shapes_mask = shapes == 1\n",
    "    cob_preds_np = torch.round(cob_preds).numpy()\n",
    "    cob_mask1 = cob_preds_np == 1\n",
    "    cob_mask0 = cob_preds_np == 0\n",
    "    intersection1 = np.logical_and(cob_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(cob_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(cob_mask1, shapes_mask).sum()\n",
    "        cob_mIoU_running += intersection1 / union\n",
    "        total = cob_mask1.sum() + shapes_mask.sum()\n",
    "        cob_dice_running = (2 * intersection1) / total\n",
    "    else:\n",
    "        union = np.logical_or(cob_mask0, shapes_mask).sum()\n",
    "        cob_mIoU_running += intersection0 / union\n",
    "        total = cob_mask0.sum() + shapes_mask.sum()\n",
    "        cob_dice_running = (2 * intersection1) / total\n",
    "    \n",
    "    correct1 = (cob_mask1 == shapes_mask).sum()\n",
    "    correct0 = (cob_mask0 == shapes_mask).sum()\n",
    "    \n",
    "    if correct1 > correct0:\n",
    "        pixels = np.size(cob_mask1)\n",
    "        cob_pAcc_running = correct1 / pixels\n",
    "    else:\n",
    "        pixels = np.size(cob_mask0)\n",
    "        cob_pAcc_running = correct0 / pixels\n",
    "        \n",
    "\n",
    "cob_loss = cob_loss_running / len(loader)\n",
    "cob_mIoU = cob_mIoU_running / len(loader)\n",
    "cob_dice = cob_dice_running / len(loader)\n",
    "cob_pAcc = cob_pAcc_running / len(loader)\n",
    "print(f'COB Scores: \\n -BCELoss: {cob_loss}\\n -mIoU: {cob_mIoU}\\n -Dice: {cob_dice}\\n -pAcc: {cob_pAcc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a3656-e17f-4738-b4c1-3851cd22ec7f",
   "metadata": {},
   "source": [
    "## Eval WTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8d8fa-bd3f-4d2e-96b6-ff66787032f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness', nargout=0)\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness/pff_segment', nargout=0)\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness/MEX', nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb8052-a75f-47ce-b3a3-7ffa48876a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_loss_running = 0\n",
    "wtp_mIoU_running = 0\n",
    "wtp_dice_running = 0\n",
    "wtp_pAcc_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch['meta']['path'][0]\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    # matlab function run\n",
    "    wtp_preds = np.asarray(eng.getHeatMap(image, 100))\n",
    "    wtp_preds = torch.from_numpy(wtp_preds.astype(np.float32))\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(wtp_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - wtp_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        wtp_loss_running += loss1.item()\n",
    "    else:\n",
    "        wtp_loss_running += loss0.item()\n",
    "        \n",
    "    # eval with mIoU and Dice\n",
    "    shapes_mask = shapes == 1\n",
    "    wtp_preds_np = torch.round(wtp_preds).numpy()\n",
    "    wtp_mask1 = wtp_preds_np == 1\n",
    "    wtp_mask0 = wtp_preds_np == 0\n",
    "    intersection1 = np.logical_and(wtp_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(wtp_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(wtp_mask1, shapes_mask).sum()\n",
    "        wtp_mIoU_running += intersection1 / union\n",
    "        total = wtp_mask1.sum() + shapes_mask.sum()\n",
    "        wtp_dice_running = (2 * intersection1) / total\n",
    "        \n",
    "    else:\n",
    "        union = np.logical_or(wtp_mask0, shapes_mask).sum()\n",
    "        wtp_mIoU_running += intersection0 / union\n",
    "        total = wtp_mask0.sum() + shapes_mask.sum()\n",
    "        wtp_dice_running = (2 * intersection0) / total\n",
    "        \n",
    "    correct1 = (wtp_mask1 == shapes_mask).sum()\n",
    "    correct0 = (wtp_mask0 == shapes_mask).sum()\n",
    "    \n",
    "    if correct1 > correct0:\n",
    "        pixels = np.size(wtp_mask1)\n",
    "        wtp_pAcc_running = correct1 / pixels\n",
    "    else:\n",
    "        pixels = np.size(wtp_mask0)\n",
    "        wtp_pAcc_running = correct0 / pixels\n",
    "        \n",
    "    \n",
    "wtp_loss = wtp_loss_running / len(loader)\n",
    "wtp_mIoU = wtp_mIoU_running / len(loader)\n",
    "wtp_dice = wtp_dice_running / len(loader)\n",
    "wtp_pAcc = wtp_pAcc_running / len(loader)\n",
    "print(f'WTP Scores: \\n -BCELoss: {wtp_loss}\\n -mIoU: {wtp_mIoU}\\n -Dice: {wtp_dice}\\n -pAcc: {wtp_pAcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd9ba2-7010-4b5e-bee3-4a6961e8f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4651b-c409-4c67-b131-621a68e4d7c3",
   "metadata": {},
   "source": [
    "## Eval CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5273a-ac17-4f4d-a75d-70037b888f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "from torchcam.cams import SmoothGradCAMpp\n",
    "\n",
    "cam_model = resnet50(pretrained = True).eval()\n",
    "cam_extractor = SmoothGradCAMpp(cam_model)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = [0.492, 0.475, 0.430], std = [0.176, 0.173, 0.176])])\n",
    "resize = transforms.Resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4017b64-9339-48a7-a471-c16f56d6125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_loss_running = 0\n",
    "cam_mIoU_running = 0\n",
    "cam_dice_running = 0\n",
    "cam_pAcc_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch['images'].squeeze().numpy()\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    image = transform(image)\n",
    "    \n",
    "    # run estimation method\n",
    "    out = cam_model(image.unsequeeze(0)) # needs to return [height x width] matrix in 0 - 1 range (sigmoid) as tensor\n",
    "    cam_preds = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "    res = resize(cam_preds.unsqueeze(0)).squeeze()\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(cam_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - cam_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        cam_loss_running += loss1.item()\n",
    "    else:\n",
    "        cam_loss_running += loss0.item()\n",
    "        \n",
    "    # eval with mIoU\n",
    "    shapes_mask = shapes == 1\n",
    "    cam_preds_np = torch.round(cam_preds).numpy()\n",
    "    cam_mask1 = cam_preds_np == 1\n",
    "    cam_mask0 = cam_preds_np == 0\n",
    "    intersection1 = np.logical_and(cam_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(cam_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(cam_mask1, shapes_mask).sum()\n",
    "        cam_mIoU_running += intersection1 / union\n",
    "        total = cam_mask1.sum() + shapes_mask.sum()\n",
    "        cam_dice_running = (2 * intersection1) / total\n",
    "    else:\n",
    "        union = np.logical_or(cam_mask0, shapes_mask).sum()\n",
    "        cam_mIoU_running += intersection0 / union\n",
    "        total = cam_mask0.sum() + shapes_mask.sum()\n",
    "        cam_dice_running = (2 * intersection0) / total\n",
    "        \n",
    "    correct1 = (cam_mask1 == shapes_mask).sum()\n",
    "    correct0 = (cam_mask0 == shapes_mask).sum()\n",
    "    \n",
    "    if correct1 > correct0:\n",
    "        pixels = np.size(cam_mask1)\n",
    "        cam_pAcc_running = correct1 / pixels\n",
    "    else:\n",
    "        pixels = np.size(cam_mask0)\n",
    "        cam_pAcc_running = correct0 / pixels\n",
    "        \n",
    "    \n",
    "cam_loss = cam_loss_running / len(loader)\n",
    "cam_mIoU = cam_mIoU_running / len(loader)\n",
    "cam_dice = cam_dice_running / len(loader)\n",
    "cam_pAcc = cam_pAcc_running / len(loader)\n",
    "print(f'CAM Scores: \\n -BCELoss: {cam_loss}\\n -mIoU: {cam_mIoU}\\n -Dice: {cam_dice}\\n -pAcc: {cam_pAcc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351253c-c1c5-4d30-acd7-aba37cae5080",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# EXPERIMENTATION ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3a05ba-ef55-441a-bb24-b8493c642267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "batch = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295e2f9c-ffdf-4653-a9c8-cb8e0847eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch[\"images\"].squeeze().numpy()\n",
    "#shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "labels = batch[\"label\"].long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6878783-0be6-495d-bbae-ad14dc74712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_mean = [0.492, 0.475, 0.430] # from preprocessing\n",
    "transform_std = [0.176, 0.173, 0.176]\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = transform_mean, std = transform_std)])\n",
    "resize = transforms.Resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef3f6f-d095-4227-a4a3-bc4d4a4e5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transform(image)\n",
    "out = cam_model(image.unsqueeze(0))\n",
    "cam_preds = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "res = resize(cam_preds.unsqueeze(0)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4650d27-b4a1-4d95-ada6-148eecf1b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize CAM prediction\n",
    "from matplotlib import cm\n",
    "Image.fromarray(np.uint8(cm.jet(res) * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb324a4-74c4-452e-bd91-5deab4ee14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize objectness measure heatmap\n",
    "for image in images:\n",
    "    name = image+\".jpg\"\n",
    "    path = os.path.join(\"/home/jovyan/work/mydata/DENMARK/512x512/images\", name)\n",
    "    wtp_preds = np.asarray(eng.getHeatMap(path, 100))\n",
    "    img = Image.fromarray(np.uint8(wtp_preds * 255), 'L')\n",
    "    img.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57ac96-7d72-43e8-9722-d3849f4e9c2a",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# OPTIONAL: TRAIN OWN MODEL FOR CAM PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3ed3b1-507d-40e3-b5de-3f16cafb2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "import datasets\n",
    "from helpers import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915c1d96-9e83-4b8a-a952-d23057e496c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set of 1700 batches and validation set of 212 batches...\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/home/jovyan/work/processed/256x256\"\n",
    "object_type = \"buildings\"\n",
    "\n",
    "images_path = os.path.join(DATA_PATH, 'image_sets_'+object_type, 'all.txt')\n",
    "images_list = [name.replace(\"\\n\",\"\") for name in io.readText(images_path)]\n",
    "\n",
    "train_size = round(len(images_list) * 0.8)\n",
    "train_images = images_list[:train_size]\n",
    "val_size = round(len(images_list) * 0.1)\n",
    "val_images = images_list[train_size:(train_size + val_size)]\n",
    "\n",
    "# create transformation object\n",
    "transform_mean = [0.492, 0.475, 0.430] # from preprocessing\n",
    "transform_std = [0.176, 0.173, 0.176]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = transform_mean, \n",
    "                                                     std = transform_std)])\n",
    "\n",
    "train_set = datasets.getDataset(name = \"denmark_points\", \n",
    "                              path = DATA_PATH,\n",
    "                              images = train_images,\n",
    "                              object_type = object_type,\n",
    "                              n_classes = 2,\n",
    "                              transform = transform)\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_loader = DataLoader(train_set, sampler = train_sampler, batch_size = 64, drop_last = True, num_workers = 1)\n",
    "\n",
    "val_set = datasets.getDataset(name = \"denmark_points\", \n",
    "                              path = DATA_PATH,\n",
    "                              images = val_images,\n",
    "                              object_type = object_type,\n",
    "                              n_classes = 2,\n",
    "                              transform = transform)\n",
    "\n",
    "val_sampler = torch.utils.data.RandomSampler(val_set)\n",
    "val_loader = DataLoader(val_set, sampler = val_sampler, batch_size = 64, drop_last = True, num_workers = 1)\n",
    "\n",
    "print(f\"Loaded training set of {len(train_loader)} batches and validation set of {len(val_loader)} batches...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e09a3d9-ae5f-4bec-ac95-92e69115a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained = True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = torch.nn.Sequential(torch.nn.Linear(2048, 512),\n",
    "                         torch.nn.ReLU(),\n",
    "                         torch.nn.Dropout(0.2),\n",
    "                         torch.nn.Linear(512, 2),\n",
    "                         torch.nn.LogSoftmax(dim=1))\n",
    "model.to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65965ce-1561-40db-b638-6201bec7bf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af743c767d504b43b3600099a94e328a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8bc6d4aed44fc09df4180d04df1053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 25\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    running_loss = 0\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        \n",
    "        inputs = batch[\"images\"].cuda()\n",
    "        labels = batch[\"label\"].long().cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            \n",
    "            inputs = batch[\"images\"].cuda()\n",
    "            labels = batch[\"label\"].long().cuda()\n",
    "            \n",
    "            out = model.forward(inputs)\n",
    "            batch_loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "            ps = torch.exp(out)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader)) \n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Train loss: {running_loss/len(train_loader):.3f}.. \"\n",
    "          f\"Test loss: {val_loss/len(val_loader):.3f}.. \"\n",
    "          f\"Test accuracy: {accuracy/len(val_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6a0e0-c8ae-4536-82ce-b31a2c14ccea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
