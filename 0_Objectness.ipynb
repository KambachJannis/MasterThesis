{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322d4e2c-4ff5-4ec2-84b6-9b37aedcdc8d",
   "metadata": {},
   "source": [
    "# OBJECTNESS EVALUATION\n",
    "\n",
    "The purpose of this Notebook is to evaluate the performance of different generic object proposal approaches on high-resultion satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b12c7-c2b0-4a85-9ab5-8827e66b74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from helpers import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af140d-0fc7-403d-b7bf-5b04b78e2750",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba53d2-0b4d-454a-83f5-877230351606",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/jovyan/work/mydata/DENMARK/512x512\"\n",
    "VOCpath = \"/home/jovyan/work/mydata/VOCdevkit/VOC2012\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98059296-696d-49f4-bfeb-5a01c5296b2e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96db815-55a5-4adf-9d2b-df175ab37fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_path = os.path.join(DATA_PATH, 'image_sets_trees', 'eval_shapes.txt')\n",
    "#images_path = os.path.join(DATA_PATH, 'image_sets_buildings', 'eval_shapes.txt')\n",
    "images_path = os.path.join(VOCpath, 'ImageSets/Segmentation', 'train_20.txt')\n",
    "\n",
    "images = [name.replace(\"\\n\",\"\") for name in io.readText(images_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f0912-c750-448c-9117-6b53d3159531",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.getDataset(name = \"denmark_shapes\", #or voc\n",
    "                              path = DATA_PATH,\n",
    "                              images = images,\n",
    "                              object_type = 'trees_eval', #trees_eval or buildings\n",
    "                              n_classes = 2,\n",
    "                              transform = None)\n",
    "\n",
    "dataset = datasets.getDataset(name = \"voc\", path = VOCpath, images = images, object_type = None, n_classes = None, transform = None)\n",
    "\n",
    "sampler = torch.utils.data.RandomSampler(dataset)\n",
    "loader = DataLoader(dataset, sampler = sampler, batch_size = 1, drop_last = True, num_workers = 1)\n",
    "\n",
    "print(f\"Loaded eval set of {len(loader)} images...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c9a3b-3ad1-42cf-a1b9-29bdde2beaa2",
   "metadata": {},
   "source": [
    "## Eval COB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f730e-38e6-4e66-bf01-e9201adc1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cobnet import COBNet\n",
    "from helpers.cob.dataset import COBtransform\n",
    "\n",
    "means = [0.492, 0.475, 0.430]\n",
    "stds = [0.176, 0.173, 0.176]\n",
    "transform = COBtransform(means, stds)\n",
    "\n",
    "cob_model = COBNet()\n",
    "cob_model.load_state_dict(torch.load(\"/home/jovyan/work/runs/X_COBNET/cp_or.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624537a5-be91-4019-a949-82049ca37257",
   "metadata": {},
   "outputs": [],
   "source": [
    "cob_loss_running = 0\n",
    "cob_mIoU_running = 0\n",
    "cob_dice_running = 0\n",
    "cob_pAcc_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch[\"images\"].numpy()\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    # transforms\n",
    "    image_cob = transform(images = image)[0]\n",
    "    image_cob = np.stack((image_cob[:,:,0], image_cob[:,:,1], image_cob[:,:,2]), axis = 0)\n",
    "    cob_tensor = torch.tensor(image_cob[np.newaxis, ...]).float()\n",
    "    \n",
    "    # model run\n",
    "    cob_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_cob = cob_model(cob_tensor)\n",
    "    \n",
    "    cob_preds = out_cob['y_fine'].sigmoid().squeeze()\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(cob_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - cob_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        cob_loss_running += loss1.item()\n",
    "    else:\n",
    "        cob_loss_running += loss0.item()\n",
    "    \n",
    "    # eval with mIoU\n",
    "    shapes_mask = shapes == 1\n",
    "    cob_preds_np = torch.round(cob_preds).numpy()\n",
    "    cob_mask1 = cob_preds_np == 1\n",
    "    cob_mask0 = cob_preds_np == 0\n",
    "    intersection1 = np.logical_and(cob_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(cob_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(cob_mask1, shapes_mask).sum()\n",
    "        cob_mIoU_running += intersection1 / union\n",
    "        total = cob_mask1.sum() + shapes_mask.sum()\n",
    "        cob_dice_running = (2 * intersection1) / total\n",
    "    else:\n",
    "        union = np.logical_or(cob_mask0, shapes_mask).sum()\n",
    "        cob_mIoU_running += intersection0 / union\n",
    "        total = cob_mask0.sum() + shapes_mask.sum()\n",
    "        cob_dice_running = (2 * intersection1) / total\n",
    "    \n",
    "    correct1 = (cob_mask1 == shapes_mask).sum()\n",
    "    correct0 = (cob_mask0 == shapes_mask).sum()\n",
    "    \n",
    "    if correct1 > correct0:\n",
    "        pixels = np.size(cob_mask1)\n",
    "        cob_pAcc_running = correct1 / pixels\n",
    "    else:\n",
    "        pixels = np.size(cob_mask0)\n",
    "        cob_pAcc_running = correct0 / pixels\n",
    "        \n",
    "\n",
    "cob_loss = cob_loss_running / len(loader)\n",
    "cob_mIoU = cob_mIoU_running / len(loader)\n",
    "cob_dice = cob_dice_running / len(loader)\n",
    "cob_pAcc = cob_pAcc_running / len(loader)\n",
    "print(f'COB Scores: \\n -BCELoss: {cob_loss}\\n -mIoU: {cob_mIoU}\\n -Dice: {cob_dice}\\n -pAcc: {cob_pAcc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a3656-e17f-4738-b4c1-3851cd22ec7f",
   "metadata": {},
   "source": [
    "## Eval WTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8d8fa-bd3f-4d2e-96b6-ff66787032f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness', nargout=0)\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness/pff_segment', nargout=0)\n",
    "eng.addpath('/home/jovyan/work/ma/helpers/objectness/MEX', nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb8052-a75f-47ce-b3a3-7ffa48876a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_loss_running = 0\n",
    "wtp_mIoU_running = 0\n",
    "wtp_dice_running = 0\n",
    "wtp_pAcc_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch['meta']['path'][0]\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    \n",
    "    # matlab function run\n",
    "    wtp_preds = np.asarray(eng.getHeatMap(image, 100))\n",
    "    wtp_preds = torch.from_numpy(wtp_preds.astype(np.float32))\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(wtp_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - wtp_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        wtp_loss_running += loss1.item()\n",
    "    else:\n",
    "        wtp_loss_running += loss0.item()\n",
    "        \n",
    "    # eval with mIoU and Dice\n",
    "    shapes_mask = shapes == 1\n",
    "    wtp_preds_np = torch.round(wtp_preds).numpy()\n",
    "    wtp_mask1 = wtp_preds_np == 1\n",
    "    wtp_mask0 = wtp_preds_np == 0\n",
    "    intersection1 = np.logical_and(wtp_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(wtp_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(wtp_mask1, shapes_mask).sum()\n",
    "        wtp_mIoU_running += intersection1 / union\n",
    "        total = wtp_mask1.sum() + shapes_mask.sum()\n",
    "        wtp_dice_running = (2 * intersection1) / total\n",
    "        \n",
    "    else:\n",
    "        union = np.logical_or(wtp_mask0, shapes_mask).sum()\n",
    "        wtp_mIoU_running += intersection0 / union\n",
    "        total = wtp_mask0.sum() + shapes_mask.sum()\n",
    "        wtp_dice_running = (2 * intersection0) / total\n",
    "        \n",
    "    correct1 = (wtp_mask1 == shapes_mask).sum()\n",
    "    correct0 = (wtp_mask0 == shapes_mask).sum()\n",
    "    \n",
    "    if correct1 > correct0:\n",
    "        pixels = np.size(wtp_mask1)\n",
    "        wtp_pAcc_running = correct1 / pixels\n",
    "    else:\n",
    "        pixels = np.size(wtp_mask0)\n",
    "        wtp_pAcc_running = correct0 / pixels\n",
    "        \n",
    "    \n",
    "wtp_loss = wtp_loss_running / len(loader)\n",
    "wtp_mIoU = wtp_mIoU_running / len(loader)\n",
    "wtp_dice = wtp_dice_running / len(loader)\n",
    "wtp_pAcc = wtp_pAcc_running / len(loader)\n",
    "print(f'WTP Scores: \\n -BCELoss: {wtp_loss}\\n -mIoU: {wtp_mIoU}\\n -Dice: {wtp_dice}\\n -pAcc: {wtp_pAcc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd9ba2-7010-4b5e-bee3-4a6961e8f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4651b-c409-4c67-b131-621a68e4d7c3",
   "metadata": {},
   "source": [
    "## Eval CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5273a-ac17-4f4d-a75d-70037b888f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "from torchcam.cams import SmoothGradCAMpp\n",
    "\n",
    "cam_model = resnet50(pretrained = True)\n",
    "cam_model.fc = torch.nn.Sequential(torch.nn.Linear(2048, 512),\n",
    "                                   torch.nn.ReLU(),\n",
    "                                   torch.nn.Dropout(0.2),\n",
    "                                   torch.nn.Linear(512, 2),\n",
    "                                   torch.nn.LogSoftmax(dim=1))\n",
    "cam_model.to(\"cuda\")\n",
    "\n",
    "cam_model.load_state_dict(torch.load(\"/home/jovyan/work/runs/X_CAMNET/cam_model_trees.pth\")) #make sure to load correct model here\n",
    "\n",
    "cam_extractor = SmoothGradCAMpp(cam_model)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = [0.492, 0.475, 0.430], std = [0.176, 0.173, 0.176])])\n",
    "resize = transforms.Resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4017b64-9339-48a7-a471-c16f56d6125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_loss_running = 0\n",
    "cam_mIoU_running = 0\n",
    "cam_dice_running = 0\n",
    "cam_pAcc_running = 0\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    image = batch['images'].squeeze().numpy()\n",
    "    shapes = batch[\"shapes\"].squeeze().numpy()\n",
    "    image = transform(image).cuda()\n",
    "    \n",
    "    # run estimation method (needs to return [height x width] matrix in 0 - 1 range (sigmoid) as tensor)\n",
    "    out = cam_model(image.unsqueeze(0))\n",
    "    cam_preds = cam_extractor(out.squeeze(0).argmax().item(), out) \n",
    "    #print(image.shape)\n",
    "    resize = transforms.Resize((len(image[0]), len(image[0][0])))\n",
    "    cam_preds = resize(cam_preds.unsqueeze(0)).squeeze().cpu()\n",
    "    \n",
    "    # save image\n",
    "    #img = Image.fromarray(np.uint8(cm.jet(cam_preds.cpu()) * 255))\n",
    "    #img.save()\n",
    "    \n",
    "    # eval with BCELoss\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss1 = criterion(cam_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    loss0 = criterion(1 - cam_preds, torch.from_numpy(shapes.astype(np.float32)))\n",
    "    ### NOTE: shape area should be as homogeneous as possible, not as low/high as possible -> take better score\n",
    "    if loss1 < loss0:\n",
    "        cam_loss_running += loss1.item()\n",
    "    else:\n",
    "        cam_loss_running += loss0.item()\n",
    "        \n",
    "    # eval with mIoU\n",
    "    shapes_mask = shapes == 1\n",
    "    cam_preds_np = torch.round(cam_preds).numpy()\n",
    "    cam_mask1 = cam_preds_np == 1\n",
    "    cam_mask0 = cam_preds_np == 0\n",
    "    intersection1 = np.logical_and(cam_mask1, shapes_mask).sum()\n",
    "    intersection0 = np.logical_and(cam_mask0, shapes_mask).sum()\n",
    "    \n",
    "    if intersection1 > intersection0:\n",
    "        union = np.logical_or(cam_mask1, shapes_mask).sum()\n",
    "        cam_mIoU_running += intersection1 / union\n",
    "        total = cam_mask1.sum() + shapes_mask.sum()\n",
    "        cam_dice_running = (2 * intersection1) / total\n",
    "    else:\n",
    "        union = np.logical_or(cam_mask0, shapes_mask).sum()\n",
    "        cam_mIoU_running += intersection0 / union\n",
    "        total = cam_mask0.sum() + shapes_mask.sum()\n",
    "        cam_dice_running = (2 * intersection0) / total\n",
    "        \n",
    "    correct1 = (cam_mask1 == shapes_mask).sum()\n",
    "    correct0 = (cam_mask0 == shapes_mask).sum()\n",
    "    \n",
    "    if correct1 > correct0:\n",
    "        pixels = np.size(cam_mask1)\n",
    "        cam_pAcc_running = correct1 / pixels\n",
    "    else:\n",
    "        pixels = np.size(cam_mask0)\n",
    "        cam_pAcc_running = correct0 / pixels\n",
    "        \n",
    "    \n",
    "cam_loss = cam_loss_running / len(loader)\n",
    "cam_mIoU = cam_mIoU_running / len(loader)\n",
    "cam_dice = cam_dice_running / len(loader)\n",
    "cam_pAcc = cam_pAcc_running / len(loader)\n",
    "print(f'CAM Scores: \\n -BCELoss: {cam_loss}\\n -mIoU: {cam_mIoU}\\n -Dice: {cam_dice}\\n -pAcc: {cam_pAcc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351253c-c1c5-4d30-acd7-aba37cae5080",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# EXPERIMENTATION ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a05ba-ef55-441a-bb24-b8493c642267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(loader)\n",
    "batch = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e2f9c-ffdf-4653-a9c8-cb8e0847eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_src = batch[\"images\"].squeeze().numpy()\n",
    "shapes = batch[\"shapes\"].squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4650d27-b4a1-4d95-ada6-148eecf1b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from skimage.io import imread\n",
    "# to visualize CAM prediction\n",
    "for image in images:\n",
    "    name = image+\".jpg\"\n",
    "    path = os.path.join(\"/home/jovyan/work/mydata/DENMARK/512x512/images\", name)\n",
    "    image_src = imread(path)\n",
    "    image = transform(image_src).cuda()\n",
    "    out = cam_model(image.unsqueeze(0))\n",
    "    cam_preds = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "    res = resize(cam_preds.unsqueeze(0)).squeeze()\n",
    "    img = Image.fromarray(np.uint8(cm.jet(res.cpu()) * 255)).convert('RGB')\n",
    "    img.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb324a4-74c4-452e-bd91-5deab4ee14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize objectness measure heatmap\n",
    "for image in images:\n",
    "    name = image+\".jpg\"\n",
    "    path = os.path.join(\"/home/jovyan/work/mydata/DENMARK/512x512/images\", name)\n",
    "    wtp_preds = np.asarray(eng.getHeatMap(path, 100))\n",
    "    img = Image.fromarray(np.uint8(wtp_preds * 255), 'L')\n",
    "    img.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57ac96-7d72-43e8-9722-d3849f4e9c2a",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# OPTIONAL: TRAIN OWN MODEL FOR CAM PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ed3b1-507d-40e3-b5de-3f16cafb2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "import datasets\n",
    "from helpers import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c1d96-9e83-4b8a-a952-d23057e496c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/jovyan/work/processed/256x256\"\n",
    "object_type = \"trees\"\n",
    "\n",
    "images_path = os.path.join(DATA_PATH, 'image_sets_'+object_type, 'all.txt')\n",
    "images_list = [name.replace(\"\\n\",\"\") for name in io.readText(images_path)]\n",
    "\n",
    "train_size = round(len(images_list) * 0.8)\n",
    "train_images = images_list[:train_size]\n",
    "val_size = round(len(images_list) * 0.1)\n",
    "val_images = images_list[train_size:(train_size + val_size)]\n",
    "\n",
    "# create transformation object\n",
    "transform_mean = [0.492, 0.475, 0.430] # from preprocessing\n",
    "transform_std = [0.176, 0.173, 0.176]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = transform_mean, \n",
    "                                                     std = transform_std)])\n",
    "\n",
    "train_set = datasets.getDataset(name = \"denmark_points\", \n",
    "                              path = DATA_PATH,\n",
    "                              images = train_images,\n",
    "                              object_type = object_type,\n",
    "                              n_classes = 2,\n",
    "                              transform = transform)\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_loader = DataLoader(train_set, sampler = train_sampler, batch_size = 64, drop_last = True, num_workers = 1)\n",
    "\n",
    "val_set = datasets.getDataset(name = \"denmark_points\", \n",
    "                              path = DATA_PATH,\n",
    "                              images = val_images,\n",
    "                              object_type = object_type,\n",
    "                              n_classes = 2,\n",
    "                              transform = transform)\n",
    "\n",
    "val_sampler = torch.utils.data.RandomSampler(val_set)\n",
    "val_loader = DataLoader(val_set, sampler = val_sampler, batch_size = 64, drop_last = True, num_workers = 1)\n",
    "\n",
    "print(f\"Loaded training set of {len(train_loader)} batches and validation set of {len(val_loader)} batches...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09a3d9-ae5f-4bec-ac95-92e69115a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained = True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = torch.nn.Sequential(torch.nn.Linear(2048, 512),\n",
    "                         torch.nn.ReLU(),\n",
    "                         torch.nn.Dropout(0.2),\n",
    "                         torch.nn.Linear(512, 2),\n",
    "                         torch.nn.LogSoftmax(dim=1))\n",
    "model.to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65965ce-1561-40db-b638-6201bec7bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    running_loss = 0\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        \n",
    "        inputs = batch[\"images\"].cuda()\n",
    "        labels = batch[\"label\"].long().cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            \n",
    "            inputs = batch[\"images\"].cuda()\n",
    "            labels = batch[\"label\"].long().cuda()\n",
    "            \n",
    "            out = model.forward(inputs)\n",
    "            batch_loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "            ps = torch.exp(out)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader)) \n",
    "    \n",
    "    if epoch == 0 or (val_loss/len(val_loader)) < val_loss_best:\n",
    "        val_loss_best = val_loss/len(val_loader)\n",
    "        torch.save(model.state_dict(), \"cam_model.pth\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Train loss: {running_loss/len(train_loader):.3f}.. \"\n",
    "          f\"Test loss: {val_loss/len(val_loader):.3f}.. \"\n",
    "          f\"Test accuracy: {accuracy/len(val_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d381a-e4a4-411a-a078-e94b5f3b1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCpath = \"/home/jovyan/work/mydata/VOCdevkit/VOC2012\"\n",
    "\n",
    "images_path = os.path.join(VOCpath, 'ImageSets/Segmentation', 'train.txt')\n",
    "images = [name.replace(\"\\n\",\"\") for name in io.readText(images_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ac55b-f270-4409-8f63-bc5a34967720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import voc\n",
    "\n",
    "dataset = voc.PascalVOC(VOCpath, images[:20])\n",
    "\n",
    "sampler = torch.utils.data.RandomSampler(dataset)\n",
    "\n",
    "loader = DataLoader(dataset, sampler = sampler, batch_size = 1, drop_last = True, num_workers = 1)\n",
    "\n",
    "print(f\"Loaded eval set of {len(loader)} images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e41f92-3f14-4386-9eaf-64618e01a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(loader)\n",
    "batch = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a6f83-94f2-4b38-9ac9-372e9fbbe552",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d48738-dfdb-4abf-9be2-c256fb8c6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = batch['target'].numpy()[0]\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.fromarray(np.uint8(target[0] * 255), 'L')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b105c-d278-44ec-90f8-52b079ad1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = batch['target'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba10c89-0e08-4a3a-bb85-d89b3f7451b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target *= 255.0/target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf5ef9-1983-421d-9e3c-9010d0d39aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087d250-a267-4652-b122-6cd159e1a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(batch['images'].numpy()[0], 'RGB')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c307c1-04b0-44fc-a316-d25e1ae4167a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
