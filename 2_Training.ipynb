{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "computational-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "cudnn.benchmark = True\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# System\n",
    "import os\n",
    "import time\n",
    "import pprint\n",
    "import itertools\n",
    "# Custom\n",
    "import models\n",
    "import datasets\n",
    "from helpers import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-billion",
   "metadata": {},
   "source": [
    "# Model Training Framework\n",
    "\n",
    "This Notebook works for all model configurations detailed in the experiment settings below\n",
    "\n",
    "Experiment Settings below could eventually be removed, not the best way of putting it rn with the dictionary tbh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-kennedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1,\n",
      " 'dataset': {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
      " 'dataset_size': {'train': 'all', 'val': 'all'},\n",
      " 'lr': 1e-05,\n",
      " 'max_epoch': 5,\n",
      " 'model': {'base': 'fcn8_vgg16', 'name': 'cob'},\n",
      " 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "####################### EXPERIMENT SETTINGS ################################\n",
    "EXP_GROUPS = {}\n",
    "\n",
    "EXP_GROUPS['denmark'] = {\"dataset\": {'name': 'denmark', \n",
    "                                     'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'lcfcn',\n",
    "                                         'base': \"fcn8_vgg16\"},\n",
    "                               \"batch_size\": [1],\n",
    "                               \"max_epoch\": [5],\n",
    "                               'dataset_size': [{'train': 'all', \n",
    "                                                 'val': 'all'},],\n",
    "                               'optimizer':['adam'],\n",
    "                               'lr':[1e-5]}\n",
    "\n",
    "EXP_GROUPS['denmark_debug'] = {\"dataset\": {'name': 'denmark', \n",
    "                                           'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'lcfcn',\n",
    "                                         'base': \"fcn8_vgg16\"},\n",
    "                               \"batch_size\": [1],\n",
    "                               \"max_epoch\": [5],\n",
    "                               'dataset_size': [{'train': 10, \n",
    "                                                 'val': 5},],\n",
    "                               'optimizer':['adam'],\n",
    "                               'lr':[1e-5]}\n",
    "\n",
    "EXP_GROUPS['denmark_debug_cob'] = {\"dataset\": {'name': 'denmark', \n",
    "                                           'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'cob',\n",
    "                                         'base': \"fcn8_vgg16\"},\n",
    "                               \"batch_size\": [1],\n",
    "                               \"max_epoch\": [5],\n",
    "                               'dataset_size': [{'train': 'all', \n",
    "                                                 'val': 'all'},],\n",
    "                               'optimizer':['adam'],\n",
    "                               'lr':[1e-5]}\n",
    "\n",
    "EXP_GROUPS = {k: utils.cartesian(v) for k, v in EXP_GROUPS.items()}\n",
    "\n",
    "exp_group_list = [\"denmark_debug_cob\"]\n",
    "exp_list = []\n",
    "for exp_group_name in exp_group_list:\n",
    "    exp_list += EXP_GROUPS[exp_group_name]\n",
    "exp_dict = exp_list[0]\n",
    "    \n",
    "########################## FILE SYSTEM SETTINGS ###########################\n",
    "\n",
    "savedir_base = \"/home/jovyan/work/runs/LCFCN\"\n",
    "#datadir = \"/home/jovyan/work/data/TRANCOS\"\n",
    "datadir = \"/home/jovyan/work/DENMARK/250x250\"\n",
    "\n",
    "############################### PRINTS ####################################\n",
    "\n",
    "pprint.pprint(exp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-blend",
   "metadata": {},
   "source": [
    "## Saving Location\n",
    "\n",
    "Create new folder for the selected experiment and save the experiment dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "graduate-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared previous experiment...\n",
      "Experiment saved in /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n"
     ]
    }
   ],
   "source": [
    "exp_id = utils.hashDict(exp_dict) #generate ID by hashing experiment dict\n",
    "savedir = os.path.join(savedir_base, exp_id)\n",
    "\n",
    "# Backup and Overwrite previous experiment with same name\n",
    "utils.deleteExperiment(savedir, backup_flag = True)\n",
    "print(\"Cleared previous experiment...\")\n",
    "\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "utils.saveJSON(os.path.join(savedir, \"exp_dict.json\"), exp_dict)\n",
    "print(\"Experiment saved in %s\" % savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-tobago",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Introduce datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controversial-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.getDataset(dataset_dict = exp_dict[\"dataset\"],\n",
    "                                 split = \"train\",\n",
    "                                 datadir = datadir,\n",
    "                                 exp_dict = exp_dict,\n",
    "                                 dataset_size = exp_dict['dataset_size'])\n",
    "val_set = datasets.getDataset(dataset_dict = exp_dict[\"dataset\"],\n",
    "                               split = \"val\",\n",
    "                               datadir = datadir,\n",
    "                               exp_dict = exp_dict,\n",
    "                               dataset_size = exp_dict['dataset_size'])\n",
    "\n",
    "# find out if this makes sense\n",
    "#train_sampler = torch.utils.data.RandomSampler(train_set, replacement=True, num_samples=2*len(val_set))\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_loader = DataLoader(train_set,\n",
    "                          sampler = train_sampler,\n",
    "                          batch_size = exp_dict[\"batch_size\"], \n",
    "                          drop_last = True, \n",
    "                          num_workers = 2)\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_set)\n",
    "val_loader = DataLoader(val_set,\n",
    "                        sampler = val_sampler,\n",
    "                        batch_size = 1,\n",
    "                        num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a52ff0-82d0-469d-a769-ef1a50d667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean computation, maybe try out\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    # shape (batch_size, 3, height, width)\n",
    "    numpy_image = data['images'].numpy()\n",
    "    \n",
    "    # shape (3,)\n",
    "    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    batch_std = np.std(numpy_image, axis=(0,2,3))\n",
    "    \n",
    "    means.append(batch_mean)\n",
    "    stds.append(batch_std)\n",
    "\n",
    "# shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n",
    "means = np.array(means).mean(axis=0)\n",
    "stds = np.array(stds).mean(axis=0)\n",
    "print(f\"Means: {means} \\n SDs: {stds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-uncertainty",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Load Model and underlying base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amazing-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.getModel(model_dict = exp_dict['model'],\n",
    "                         exp_dict = exp_dict,\n",
    "                         train_set = train_set).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-despite",
   "metadata": {},
   "source": [
    "## Experiment Run Management \n",
    "\n",
    "Resume experiment if a previous score_list exists or start a new one from epoch 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acknowledged-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning new experiment from epoch 0\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(savedir, \"model.pth\")\n",
    "score_list_path = os.path.join(savedir, \"score_list.pkl\")\n",
    "\n",
    "if os.path.exists(score_list_path): #resume\n",
    "    model.loadStateDict(utils.loadTorch(model_path))\n",
    "    score_list = utils.loadPKL(score_list_path)\n",
    "    s_epoch = score_list[-1]['epoch'] + 1\n",
    "    print(f\"Resuming previous experiment fom epoch {s_epoch}\")\n",
    "else: #restart\n",
    "    score_list = []\n",
    "    s_epoch = 0\n",
    "    print(f\"Beginning new experiment from epoch {s_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-marine",
   "metadata": {},
   "source": [
    "## Main Epoch Loop\n",
    "\n",
    "Each epoch conists of training, validation, updating the statstics and saving the best as well as the most recent model and validation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "departmental-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 7.0418: 100%|██████████| 439/439 [00:30<00:00, 14.47it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.2302: 100%|██████████| 126/126 [00:04<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.230159  -2.230159    7.041775      0 \n",
      "\n",
      "Checkpoint Saved: /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best: /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n",
      "Epoch 1 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 6.8420: 100%|██████████| 439/439 [00:29<00:00, 15.01it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.3492: 100%|██████████| 126/126 [00:12<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.230159  -2.230159    7.041775      0\n",
      "1  2.349206  -2.349206    6.841991      1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Saved: /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n",
      "Epoch 2 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 6.3875: 100%|██████████| 439/439 [00:29<00:00, 14.94it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 1.4286: 100%|██████████| 126/126 [00:10<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.230159  -2.230159    7.041775      0\n",
      "1  2.349206  -2.349206    6.841991      1\n",
      "2  1.428571  -1.428571    6.387503      2 \n",
      "\n",
      "Checkpoint Saved: /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best: /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n",
      "Epoch 3 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 5.5910: 100%|██████████| 439/439 [00:29<00:00, 14.64it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 1.4603: 100%|██████████| 126/126 [00:13<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.230159  -2.230159    7.041775      0\n",
      "1  2.349206  -2.349206    6.841991      1\n",
      "2  1.428571  -1.428571    6.387503      2\n",
      "3  1.460317  -1.460317    5.591024      3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Saved: /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n",
      "Epoch 4 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 5.1228: 100%|██████████| 439/439 [00:30<00:00, 14.59it/s]\n",
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.0794: 100%|██████████| 126/126 [00:04<00:00, 29.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.230159  -2.230159    7.041775      0\n",
      "1  2.349206  -2.349206    6.841991      1\n",
      "2  1.428571  -1.428571    6.387503      2\n",
      "3  1.460317  -1.460317    5.591024      3\n",
      "4  2.079365  -2.079365    5.122836      4 \n",
      "\n",
      "Checkpoint Saved: /home/jovyan/work/runs/LCFCN/8bd70e101945e82afea8a5021b195720\n",
      "Epoch 5 of 5 completed.\n",
      "Experiment completed!\n"
     ]
    }
   ],
   "source": [
    "for e in range(s_epoch, exp_dict['max_epoch']):\n",
    "    # Validate only at the start of each cycle\n",
    "    score_dict = {}\n",
    "    # Train the model\n",
    "    train_dict = model.trainOnLoader(model, train_loader)\n",
    "    print(\"Training done...\")\n",
    "    # Validate and Visualize the model\n",
    "    val_dict = model.valOnLoader(val_loader, savedir_images=os.path.join(savedir, \"images\"), n_images=30)\n",
    "    print(\"Validation done..\")\n",
    "    \n",
    "    # Update score_dict and add to score_list\n",
    "    score_dict.update(val_dict)\n",
    "    score_dict.update(train_dict)\n",
    "    score_dict[\"epoch\"] = len(score_list)\n",
    "    score_list += [score_dict]\n",
    "\n",
    "    # Report score_list\n",
    "    score_df = pd.DataFrame(score_list)\n",
    "    print(\"\\n\", score_df.tail(), \"\\n\")\n",
    "    \n",
    "    # Save Model and score_list\n",
    "    utils.saveTorch(model_path, model.getStateDict())\n",
    "    utils.savePKL(score_list_path, score_list)\n",
    "    print(\"Checkpoint Saved: %s\" % savedir)\n",
    "\n",
    "    # Save best Checkpoint\n",
    "    if e == 0 or (score_dict.get(\"val_score\", 0) > score_df[\"val_score\"][:-1].fillna(0).max()):\n",
    "        utils.savePKL(os.path.join(savedir, \"score_list_best.pkl\"), score_list)\n",
    "        utils.saveTorch(os.path.join(savedir, \"model_best.pth\"), model.getStateDict())\n",
    "        print(\"Saved Best: %s\" % savedir)\n",
    "    print(f\"Epoch {e+1} of {exp_dict['max_epoch'] - s_epoch} completed.\")\n",
    "\n",
    "print(f\"Experiment completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ac00b-6925-434c-8414-db7b5b3e8903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
