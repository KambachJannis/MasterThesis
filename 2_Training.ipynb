{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "computational-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "cudnn.benchmark = True\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# System\n",
    "import os\n",
    "import pprint\n",
    "import itertools\n",
    "# Custom\n",
    "import models\n",
    "import datasets\n",
    "from helpers import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-billion",
   "metadata": {},
   "source": [
    "# Model Training Framework\n",
    "\n",
    "This Notebook works for all model configurations detailed in the experiment settings below\n",
    "\n",
    "Experiment Settings below could eventually be removed, not the best way of putting it rn with the dictionary tbh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-kennedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1,\n",
      " 'dataset': {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
      " 'dataset_size': {'train': 'all', 'val': 'all'},\n",
      " 'lr': 1e-05,\n",
      " 'max_epoch': 5,\n",
      " 'model': {'base': 'unet', 'name': 'supervised'},\n",
      " 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "########################## FILE SYSTEM SETTINGS ###########################\n",
    "\n",
    "savedir_base = \"/home/jovyan/work/runs/\"\n",
    "datadir = \"/home/jovyan/work/DENMARK/256x256\"\n",
    "\n",
    "####################### EXPERIMENT SETTINGS ################################\n",
    "EXP_GROUPS = {}\n",
    "\n",
    "EXP_GROUPS['denmark'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                         \"model\": {'name': 'lcfcn', 'base': \"fcn8_vgg16\"},\n",
    "                         \"batch_size\": 1,\n",
    "                         \"max_epoch\": 5,\n",
    "                         \"dataset_size\": {'train': 'all', 'val': 'all'},\n",
    "                         \"optimizer\": 'adam',\n",
    "                         \"lr\": 1e-5}\n",
    "\n",
    "EXP_GROUPS['denmark_debug'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'lcfcn', 'base': \"fcn8_vgg16\"},\n",
    "                               \"batch_size\": 1,\n",
    "                               \"max_epoch\": 5,\n",
    "                               \"dataset_size\": {'train': 10, 'val': 5},\n",
    "                               \"optimizer\": 'adam',\n",
    "                               \"lr\": 1e-5}\n",
    "\n",
    "EXP_GROUPS['denmark_cob'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                             \"model\": {'name': 'cob', 'base': \"fcn8_vgg16\"},\n",
    "                             \"batch_size\": 1,\n",
    "                             \"max_epoch\": 5,\n",
    "                             \"dataset_size\": {'train': 'all', 'val': 'all'},\n",
    "                             \"optimizer\": 'adam',\n",
    "                             \"lr\": 1e-5}\n",
    "\n",
    "EXP_GROUPS['denmark_sup'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                             \"model\": {'name': 'supervised', 'base': \"unet\"},\n",
    "                             \"batch_size\": 1,\n",
    "                             \"max_epoch\": 5,\n",
    "                             \"dataset_size\": {'train': 'all', 'val': 'all'},\n",
    "                             \"optimizer\": 'adam',\n",
    "                             \"lr\": 1e-5}\n",
    "\n",
    "############################### SELECT EXPERIMENT ####################################\n",
    "\n",
    "exp_dict = EXP_GROUPS['denmark_sup']\n",
    "pprint.pprint(exp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-blend",
   "metadata": {},
   "source": [
    "## Saving Location\n",
    "\n",
    "Create new folder for the selected experiment and save the experiment dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "graduate-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared previous experiment...\n",
      "Experiment saved in /home/jovyan/work/runs/78de28738873de9988fa6f3b768fcccd\n"
     ]
    }
   ],
   "source": [
    "exp_id = utils.hashDict(exp_dict) #generate ID by hashing experiment dict\n",
    "savedir = os.path.join(savedir_base, exp_id)\n",
    "\n",
    "# Backup and Overwrite previous experiment with same name\n",
    "utils.deleteExperiment(savedir, backup_flag = True)\n",
    "print(\"Cleared previous experiment...\")\n",
    "\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "utils.saveJSON(os.path.join(savedir, \"exp_dict.json\"), exp_dict)\n",
    "print(\"Experiment saved in %s\" % savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-tobago",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Introduce datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controversial-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.getDataset(dataset = exp_dict[\"dataset\"][\"name\"],\n",
    "                                split = \"train\",\n",
    "                                datadir = datadir,\n",
    "                                transform = exp_dict['dataset']['transform'],\n",
    "                                dataset_size = exp_dict['dataset_size'])\n",
    "\n",
    "val_set = datasets.getDataset(dataset = exp_dict[\"dataset\"][\"name\"],\n",
    "                              split = \"val\",\n",
    "                              datadir = datadir,\n",
    "                              transform = exp_dict['dataset']['transform'],\n",
    "                              dataset_size = exp_dict['dataset_size'])\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_loader = DataLoader(train_set,\n",
    "                          sampler = train_sampler,\n",
    "                          batch_size = exp_dict[\"batch_size\"], \n",
    "                          drop_last = True, \n",
    "                          num_workers = 1)\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_set)\n",
    "val_loader = DataLoader(val_set,\n",
    "                        sampler = val_sampler,\n",
    "                        batch_size = 1,\n",
    "                        num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a52ff0-82d0-469d-a769-ef1a50d667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean computation, maybe try out\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    # shape (batch_size, 3, height, width)\n",
    "    numpy_image = data['images'].numpy()\n",
    "    \n",
    "    # shape (3,)\n",
    "    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    batch_std = np.std(numpy_image, axis=(0,2,3))\n",
    "    \n",
    "    means.append(batch_mean)\n",
    "    stds.append(batch_std)\n",
    "\n",
    "# shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n",
    "means = np.array(means).mean(axis=0)\n",
    "stds = np.array(stds).mean(axis=0)\n",
    "print(f\"Means: {means} \\n SDs: {stds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-uncertainty",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Load Model and underlying base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amazing-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = train_set.n_classes + 1 #this needs to be +1 for fully supervised loss\n",
    "model = models.getModel(exp_dict, n_classes).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-despite",
   "metadata": {},
   "source": [
    "## Experiment Run Management \n",
    "\n",
    "Resume experiment if a previous score_list exists or start a new one from epoch 0 if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acknowledged-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning new experiment from epoch 0\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(savedir, \"model.pth\")\n",
    "score_list_path = os.path.join(savedir, \"score_list.pkl\")\n",
    "\n",
    "if os.path.exists(score_list_path): #resume\n",
    "    model.loadStateDict(utils.loadTorch(model_path))\n",
    "    score_list = utils.loadPKL(score_list_path)\n",
    "    s_epoch = score_list[-1]['epoch'] + 1\n",
    "    print(f\"Resuming previous experiment fom epoch {s_epoch}\")\n",
    "else: #restart\n",
    "    score_list = []\n",
    "    s_epoch = 0\n",
    "    print(f\"Beginning new experiment from epoch {s_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-marine",
   "metadata": {},
   "source": [
    "## Main Epoch Loop\n",
    "\n",
    "Each epoch conists of training, validation, updating the statstics and saving the best as well as the most recent model and validation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "departmental-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 0.3094: 100%|██████████| 451/451 [00:18<00:00, 23.82it/s]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.3798: 100%|██████████| 129/129 [00:02<00:00, 47.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.379845  -2.379845    0.309384      0 \n",
      "\n",
      "Checkpoint Saved: /home/jovyan/work/runs/78de28738873de9988fa6f3b768fcccd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/451 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best: /home/jovyan/work/runs/78de28738873de9988fa6f3b768fcccd\n",
      "Epoch 1 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 0.1737: 100%|██████████| 451/451 [00:16<00:00, 27.04it/s]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.3798: 100%|██████████| 129/129 [00:02<00:00, 47.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.379845  -2.379845    0.309384      0\n",
      "1  2.379845  -2.379845    0.173706      1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/451 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Saved: /home/jovyan/work/runs/78de28738873de9988fa6f3b768fcccd\n",
      "Epoch 2 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 0.1709: 100%|██████████| 451/451 [00:16<00:00, 27.30it/s]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.3798: 100%|██████████| 129/129 [00:02<00:00, 48.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.379845  -2.379845    0.309384      0\n",
      "1  2.379845  -2.379845    0.173706      1\n",
      "2  2.379845  -2.379845    0.170904      2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/451 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Saved: /home/jovyan/work/runs/78de28738873de9988fa6f3b768fcccd\n",
      "Epoch 3 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 0.1683: 100%|██████████| 451/451 [00:16<00:00, 27.33it/s]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.3798: 100%|██████████| 129/129 [00:02<00:00, 47.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.379845  -2.379845    0.309384      0\n",
      "1  2.379845  -2.379845    0.173706      1\n",
      "2  2.379845  -2.379845    0.170904      2\n",
      "3  2.379845  -2.379845    0.168311      3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/451 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Saved: /home/jovyan/work/runs/78de28738873de9988fa6f3b768fcccd\n",
      "Epoch 4 of 5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training. Loss: 0.1655: 100%|██████████| 451/451 [00:16<00:00, 26.92it/s]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating. MAE: 2.3798: 100%|██████████| 129/129 [00:02<00:00, 46.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation done..\n",
      "\n",
      "     val_mae  val_score  train_loss  epoch\n",
      "0  2.379845  -2.379845    0.309384      0\n",
      "1  2.379845  -2.379845    0.173706      1\n",
      "2  2.379845  -2.379845    0.170904      2\n",
      "3  2.379845  -2.379845    0.168311      3\n",
      "4  2.379845  -2.379845    0.165507      4 \n",
      "\n",
      "Checkpoint Saved: /home/jovyan/work/runs/78de28738873de9988fa6f3b768fcccd\n",
      "Epoch 5 of 5 completed.\n",
      "Experiment completed!\n"
     ]
    }
   ],
   "source": [
    "for e in range(s_epoch, exp_dict['max_epoch']):\n",
    "    # Validate only at the start of each cycle\n",
    "    score_dict = {}\n",
    "    # Train the model\n",
    "    train_dict = model.trainOnLoader(model, train_loader)\n",
    "    print(\"Training done...\")\n",
    "    # Validate and Visualize the model\n",
    "    val_dict = model.valOnLoader(val_loader, savedir_images=os.path.join(savedir, \"images\"), n_images=30)\n",
    "    print(\"Validation done..\")\n",
    "    \n",
    "    # Update score_dict and add to score_list\n",
    "    score_dict.update(val_dict)\n",
    "    score_dict.update(train_dict)\n",
    "    score_dict[\"epoch\"] = len(score_list)\n",
    "    score_list += [score_dict]\n",
    "\n",
    "    # Report score_list\n",
    "    score_df = pd.DataFrame(score_list)\n",
    "    print(\"\\n\", score_df.tail(), \"\\n\")\n",
    "    \n",
    "    # Save Model and score_list\n",
    "    utils.saveTorch(model_path, model.getStateDict())\n",
    "    utils.savePKL(score_list_path, score_list)\n",
    "    print(\"Checkpoint Saved: %s\" % savedir)\n",
    "\n",
    "    # Save best Checkpoint\n",
    "    if e == 0 or (score_dict.get(\"val_score\", 0) > score_df[\"val_score\"][:-1].fillna(0).max()):\n",
    "        utils.savePKL(os.path.join(savedir, \"score_list_best.pkl\"), score_list)\n",
    "        utils.saveTorch(os.path.join(savedir, \"model_best.pth\"), model.getStateDict())\n",
    "        print(\"Saved Best: %s\" % savedir)\n",
    "    \n",
    "    print(f\"Epoch {e+1} of {exp_dict['max_epoch'] - s_epoch} completed.\")\n",
    "\n",
    "print(f\"Experiment completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e037cd8-ba43-4198-b8b1-d911ac2ae3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "probs = torch.load('probs.pt').cpu()#.detach().numpy()\n",
    "probs2 = torch.load('probs2.pt').cpu()#.detach().numpy()\n",
    "shape = torch.load('shape.pt').cpu()#.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348a570-a61f-462a-bc91-3890392b96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef4416-752d-4258-9f45-50f5d3999ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4c6e0-cfeb-457a-b8cb-95396c312f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
