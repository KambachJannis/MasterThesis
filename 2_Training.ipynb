{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a65fbe-d6c0-45d8-aa4b-6dc201b1760a",
   "metadata": {},
   "source": [
    "# MODEL TRAINING\n",
    "\n",
    "The purpose of this notebook is to test different model learning configurations. Outputs (model checkpoints, configuration dictionary) are saved in the specified directory below. Data path should lead to output folder created during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "computational-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.backends import cudnn \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "cudnn.benchmark = True # might speed up runtime, test later or leave out\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import models\n",
    "import losses\n",
    "import datasets\n",
    "from helpers import io, trainer, run_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b63704-ad47-438a-a5c1-dc28e8488873",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/jovyan/work/DENMARK/256x256\"\n",
    "SAVE_PATH = \"/home/jovyan/work/runs/MODELS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a4b8c8-eb48-4067-a978-c78e7bb35f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ID = \"sup1\"\n",
    "EM = run_manager.Manager(EXP_ID, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-billion",
   "metadata": {},
   "source": [
    "\n",
    "## Data\n",
    "\n",
    "Dataset class takes list of image names as _images_ parameter. These lists for training and validation sets are created in the next cell from .txt files created during preprocessing in the _image_sets_ folder.\n",
    "\n",
    "Preporcessing assignes image names to files based on what label-information is available for each image. This is done to get a predictable output for the dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b8a681-b6ac-4754-ad34-706aafebbea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: \n",
      " - train: 265 \n",
      " - val: 76\n"
     ]
    }
   ],
   "source": [
    "# basic settings for dataset\n",
    "EM.dataset_type = \"denmark_shapes\"\n",
    "EM.batch_size_train = 1\n",
    "EM.batch_size_val = 1\n",
    "EM.n_classes = 2\n",
    "\n",
    "# load image-lists from files\n",
    "images_path = os.path.join(DATA_PATH, 'image_sets', 'shapes.txt')\n",
    "images_list = [name.replace(\"\\n\",\"\") for name in io.readText(images_path)]\n",
    "train_size = round(len(images_list) * 0.7)\n",
    "train_images = images_list[:train_size]\n",
    "val_size = round(len(images_list) * 0.2)\n",
    "val_images = images_list[train_size:(train_size + val_size)]\n",
    "\n",
    "# create transformation object\n",
    "transform_mean = [0.492, 0.475, 0.430]\n",
    "transform_std = [0.176, 0.173, 0.176]\n",
    "\n",
    "EM.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean = transform_mean, \n",
    "                                                        std = transform_std)])\n",
    "\n",
    "print(f\"Dataset sizes: \\n - train: {len(train_images)} \\n - val: {len(val_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controversial-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.getDataset(name = EM.dataset_type,\n",
    "                                path = DATA_PATH,\n",
    "                                images = train_images,\n",
    "                                n_classes = EM.n_classes,\n",
    "                                transform = EM.transform)\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "\n",
    "train_loader = DataLoader(train_set, sampler = train_sampler,\n",
    "                          batch_size = EM.batch_size_train, \n",
    "                          drop_last = True, num_workers = 2)\n",
    "\n",
    "val_set = datasets.getDataset(name = EM.dataset_type,\n",
    "                              path = DATA_PATH,\n",
    "                              images = val_images,\n",
    "                              n_classes = EM.n_classes,\n",
    "                              transform = EM.transform)\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_set)\n",
    "\n",
    "val_loader = DataLoader(val_set, sampler = val_sampler,\n",
    "                        batch_size = EM.batch_size_val,\n",
    "                        num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-uncertainty",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Model can be selected from _vgg16_, _resnet_ and _unet_. Available loss functions for point-supervision are _lcfcn_ and and _cob_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amazing-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic settings for model\n",
    "EM.type = 'supervised'\n",
    "EM.net_name = 'unet'\n",
    "EM.opt_name = 'sgd'\n",
    "EM.loss_name = 'CrossEntropy'\n",
    "\n",
    "# optimizer-specific settings\n",
    "EM.adam_learning_rate = 1e-5\n",
    "EM.adam_betas = (0.99, 0.999)\n",
    "EM.adam_decay = 0.0005 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31ada3e-ab93-45e1-8ce3-364f9e635df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.getNet(EM.net_name, EM.n_classes).cuda()\n",
    "\n",
    "criterion = losses.getLoss(EM.loss_name)\n",
    "\n",
    "if EM.opt_name == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = EM.adam_learning_rate, betas = EM.adam_betas, weight_decay = EM.adam_decay)\n",
    "elif EM.opt_name == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = EM.adam_learning_rate)\n",
    "else:\n",
    "    print(\"This is a sanity check for the experiment dictionary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-despite",
   "metadata": {},
   "source": [
    "## Run Management\n",
    "\n",
    "Check if a previous run with the same ID exists and either load the last state dicts or move the run folder into the backup folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acknowledged-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "EM.begin()\n",
    "\n",
    "if os.path.exists(EM.save_path):\n",
    "    confirm = input(\"Run with same ID found - load, backup or cancel?: \")\n",
    "    \n",
    "    if confirm == 'load':\n",
    "        # load state dicts\n",
    "        checkpoint = torch.load(os.path.join(EM.save_path, 'checkpoint_last.pth'))\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        # take epoch settings from manager\n",
    "        manager = io.loadPKL(os.path.join(EM.save_path, 'manager.pkl'))\n",
    "        EM.resume(manager)\n",
    "        print(f\"Loaded previous run - continuing from epoch {EM.current_epoch}...\")\n",
    "   \n",
    "    elif confirm == 'backup':\n",
    "        # move existing folder to backup subfolder\n",
    "        shutil.move(EM.save_path, os.path.join(SAVE_PATH, 'backup', EM.id))\n",
    "        print(f\"Starting new run from epoch 0...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No action taken...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-marine",
   "metadata": {},
   "source": [
    "## Main Epoch Loop\n",
    "\n",
    "Each epoch conists of training, validation, updating the statstics and saving the best as well as the most recent model and validation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "departmental-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5438a6b0ccce48d282c20de028a54e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005361893a2d47cc8db92fa528a76e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "1only batches of spatial targets supported (3D tensors) but got targets of size: : [1, 1, 256, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-aaf8e3d43a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Training Phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training done with loss: {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/ma/helpers/trainer.py\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, optimizer, train_loader, criterion)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2266\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m         \u001b[0;31m# dim == 3 or dim > 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1only batches of spatial targets supported (3D tensors) but got targets of size: : [1, 1, 256, 256]"
     ]
    }
   ],
   "source": [
    "EM.epochs = 5\n",
    "start_epoch = EM.current_epoch\n",
    "os.mkdir(EM.save_path)\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, EM.epochs)):\n",
    "    \n",
    "    # Training Phase\n",
    "    train_loss = trainer.trainModel(model, optimizer, train_loader, criterion, EM.type)\n",
    "    print(f\"Training done with loss: {train_loss}\")\n",
    "    \n",
    "    # Validation Phase\n",
    "    val_loss = trainer.valModel(model, val_loader, criterion, EM.type)\n",
    "    print(f\"Validation done with loss: {val_loss}\")\n",
    "    \n",
    "    # update experiment manager with losses\n",
    "    loss_dict = {'epoch': epoch+1, 'train': train_loss, 'val': val_loss}\n",
    "    EM.loss_list += [loss_dict]\n",
    "    EM.current_epoch = epoch\n",
    "    print(\"\\n\", pd.DataFrame(EM.loss_list).tail(), \"\\n\")\n",
    "    \n",
    "    # save model optimizer and manager as checkpoint\n",
    "    checkpoint = {'epoch': epoch+1, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, os.path.join(EM.save_path, 'checkpoint_last.pth'))\n",
    "    io.savePKL(os.path.join(EM.save_path, 'manager.pkl'), EM)\n",
    "    \n",
    "    # check if new best model\n",
    "    if epoch == 0 or train_loss < EM.best_loss:\n",
    "        shutil.copy(os.path.join(EM.save_path, 'checkpoint_last.pth'), os.path.join(EM.save_path, 'checkpoint_best.pth'))\n",
    "        EM.best_loss = train_loss\n",
    "    print(\"Checkpoint saved... \")\n",
    "\n",
    "print(f\"Run completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### EXPERIMENT SETTINGS ################################\n",
    "EXP_GROUPS = {}\n",
    "\n",
    "EXP_GROUPS['denmark'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                         \"model\": {'name': 'lcfcn', 'base': \"fcn8_vgg16\"},\n",
    "                         \"batch_size\": 1,\n",
    "                         \"max_epoch\": 5,\n",
    "                         \"dataset_size\": {'train': 'all', 'val': 'all'},\n",
    "                         \"optimizer\": 'adam',\n",
    "                         \"lr\": 1e-5}\n",
    "\n",
    "EXP_GROUPS['denmark_debug'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                               \"model\": {'name': 'lcfcn', 'base': \"fcn8_vgg16\"},\n",
    "                               \"batch_size\": 1,\n",
    "                               \"max_epoch\": 5,\n",
    "                               \"dataset_size\": {'train': 10, 'val': 5},\n",
    "                               \"optimizer\": 'adam',\n",
    "                               \"lr\": 1e-5}\n",
    "\n",
    "EXP_GROUPS['denmark_cob'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                             \"model\": {'name': 'cob', 'base': \"fcn8_vgg16\"},\n",
    "                             \"batch_size\": 1,\n",
    "                             \"max_epoch\": 5,\n",
    "                             \"dataset_size\": {'train': 'all', 'val': 'all'},\n",
    "                             \"optimizer\": 'adam',\n",
    "                             \"lr\": 1e-5}\n",
    "\n",
    "EXP_GROUPS['denmark_sup'] = {\"dataset\": {'name': 'denmark', 'transform': 'rgb_normalize'},\n",
    "                             \"model\": {'name': 'supervised', 'base': \"unet\"},\n",
    "                             \"batch_size\": 1,\n",
    "                             \"max_epoch\": 5,\n",
    "                             \"dataset_size\": {'train': 'all', 'val': 'all'},\n",
    "                             \"optimizer\": 'adam',\n",
    "                             \"lr\": 1e-5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b368a-8305-44ae-a0a5-57dd6fdbf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
